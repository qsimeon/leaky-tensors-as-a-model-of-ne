{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-ml1s5zpx-gvv1bm",
      "metadata": {},
      "source": [
        "# Leaky Tensors as a Model of Neuromodulation in Deep Networks\n",
        "\n",
        "This notebook demonstrates a novel approach to neural network training where we inject random noise into network weights at each training step. This simulates neuromodulation in biological neural networks and forces the model to learn robust representations.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Leaky Tensors: Network weights that have additive noise injected during training\n",
        "- Neuromodulation: A learned noise model with finite variance added to network units\n",
        "- Covariance Shift Robustness: The model must learn to be robust to weight perturbations\n",
        "- Noise Injection: Independent random noise added at every training step\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-1-ml1s5zpx-ezelki",
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "# Import from neural_model module\n",
        "from neural_model import (LeakyLinear, LeakyConv2d, LeakyMLP, LeakyCNN, create_model)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-2-ml1s5zpx-nopbyt",
      "metadata": {},
      "source": [
        "## Noise Model for Neuromodulation\n",
        "\n",
        "We create a learnable noise model that generates noise with finite variance. This noise model is trained alongside the main network to find optimal noise patterns that improve robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-3-ml1s5zpx-39wlwv",
      "metadata": {},
      "source": [
        "class NoiseModel(nn.Module):\n",
        "    \"\"\"Learnable noise model that generates noise with finite variance.\"\"\"\n",
        "    def __init__(self, layer_shapes: Dict[str, tuple]):\n",
        "        super(NoiseModel, self).__init__()\n",
        "        self.layer_shapes = layer_shapes\n",
        "        \n",
        "        # Learnable variance parameters for each layer (log scale for stability)\n",
        "        self.log_variances = nn.ParameterDict({\n",
        "            name: nn.Parameter(torch.zeros(1))\n",
        "            for name in layer_shapes.keys()\n",
        "        })\n",
        "    \n",
        "    def generate_noise(self) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Generate noise tensors for each layer.\"\"\"\n",
        "        noise_dict = {}\n",
        "        for name, shape in self.layer_shapes.items():\n",
        "            # Convert log variance to standard deviation\n",
        "            std = torch.exp(0.5 * self.log_variances[name])\n",
        "            # Clamp to prevent explosion\n",
        "            std = torch.clamp(std, min=1e-6, max=0.5)\n",
        "            # Generate Gaussian noise\n",
        "            noise = torch.randn(shape, device=std.device) * std\n",
        "            noise_dict[name] = noise\n",
        "        return noise_dict\n",
        "    \n",
        "    def get_variances(self) -> Dict[str, float]:\n",
        "        \"\"\"Get current variance values for monitoring.\"\"\"\n",
        "        return {name: torch.exp(log_var).item() for name, log_var in self.log_variances.items()}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-ml1s5zpx-yy952x",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "We'll use MNIST dataset for demonstration. The dataset is simple enough to train quickly while being complex enough to demonstrate the neuromodulation effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-5-ml1s5zpx-acvns0",
      "metadata": {},
      "source": [
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Training samples: {len(train_dataset)}')\n",
        "print(f'Test samples: {len(test_dataset)}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-ml1s5zpx-f37jzu",
      "metadata": {},
      "source": [
        "## Model Creation and Initialization\n",
        "\n",
        "We create a LeakyMLP model that supports noise injection. We also initialize the noise model with the appropriate shapes for each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-7-ml1s5zpx-49qrz8",
      "metadata": {},
      "source": [
        "# Create the main model using the factory function\n",
        "model = create_model(\n",
        "    model_type='mlp',\n",
        "    input_dim=28*28,\n",
        "    hidden_dims=[512, 256],\n",
        "    output_dim=10,\n",
        "    dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "# Get layer shapes for noise model\n",
        "layer_shapes = {}\n",
        "for i, layer in enumerate(model.get_leaky_layers()):\n",
        "    layer_shapes[f'layer_{i}'] = layer.weight.shape\n",
        "\n",
        "# Create noise model\n",
        "noise_model = NoiseModel(layer_shapes).to(device)\n",
        "\n",
        "print(f'Model architecture:\\n{model}')\n",
        "print(f'\\nNoise model layer shapes: {layer_shapes}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-8-ml1s5zpx-ko7eqn",
      "metadata": {},
      "source": [
        "## Training with Neuromodulation\n",
        "\n",
        "The training loop injects noise at every step, forcing the model to learn robust representations. We train both the main model and the noise model simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-9-ml1s5zpx-vzkmgv",
      "metadata": {},
      "source": [
        "def train_with_neuromodulation(model, noise_model, train_loader, epochs=5, lr=0.001):\n",
        "    \"\"\"Train model with learnable neuromodulation.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Separate optimizers for model and noise model\n",
        "    model_optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    noise_optimizer = optim.Adam(noise_model.parameters(), lr=lr * 0.1)\n",
        "    \n",
        "    history = {'train_loss': [], 'train_acc': [], 'noise_vars': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.size(0), -1)  # Flatten for MLP\n",
        "            \n",
        "            # Generate and inject noise\n",
        "            noise_dict = noise_model.generate_noise()\n",
        "            model.inject_noise(noise_dict)\n",
        "            \n",
        "            # Forward pass\n",
        "            model_optimizer.zero_grad()\n",
        "            noise_optimizer.zero_grad()\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            model_optimizer.step()\n",
        "            noise_optimizer.step()\n",
        "            \n",
        "            # Clear noise after update\n",
        "            model.clear_noise()\n",
        "            \n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "        \n",
        "        # Epoch statistics\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "        noise_vars = noise_model.get_variances()\n",
        "        \n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "        history['noise_vars'].append(noise_vars)\n",
        "        \n",
        "        print(f'\\nEpoch {epoch+1} Summary: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%')\n",
        "        print(f'Noise variances: {noise_vars}\\n')\n",
        "    \n",
        "    return history\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-10-ml1s5zpx-zvzkui",
      "metadata": {},
      "source": [
        "## Run Training\n",
        "\n",
        "Execute the training loop with neuromodulation enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-11-ml1s5zpx-jj0yr2",
      "metadata": {},
      "source": [
        "# Train the model\n",
        "history = train_with_neuromodulation(model, noise_model, train_loader, epochs=3, lr=0.001)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-12-ml1s5zpx-w7clyv",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate the model on the test set both with and without noise injection to understand the robustness gained through neuromodulation training.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-13-ml1s5zpx-asaytn",
      "metadata": {},
      "source": [
        "def evaluate(model, test_loader, with_noise=False, noise_model=None):\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.size(0), -1)\n",
        "            \n",
        "            if with_noise and noise_model is not None:\n",
        "                noise_dict = noise_model.generate_noise()\n",
        "                model.inject_noise(noise_dict)\n",
        "            \n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            if with_noise:\n",
        "                model.clear_noise()\n",
        "    \n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate without noise\n",
        "test_acc_clean = evaluate(model, test_loader, with_noise=False)\n",
        "print(f'Test Accuracy (without noise): {test_acc_clean:.2f}%')\n",
        "\n",
        "# Evaluate with noise\n",
        "test_acc_noisy = evaluate(model, test_loader, with_noise=True, noise_model=noise_model)\n",
        "print(f'Test Accuracy (with noise): {test_acc_noisy:.2f}%')\n",
        "\n",
        "# Robustness metric\n",
        "robustness = test_acc_noisy / test_acc_clean\n",
        "print(f'\\nRobustness ratio: {robustness:.4f}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-14-ml1s5zpx-yclrnj",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize the training progress and learned noise variances to understand how the neuromodulation evolved during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-15-ml1s5zpx-vcmf7w",
      "metadata": {},
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot loss\n",
        "axes[0].plot(history['train_loss'], marker='o')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "axes[1].plot(history['train_acc'], marker='o', color='green')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training Accuracy')\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot noise variances\n",
        "for layer_name in history['noise_vars'][0].keys():\n",
        "    variances = [epoch_vars[layer_name] for epoch_vars in history['noise_vars']]\n",
        "    axes[2].plot(variances, marker='o', label=layer_name)\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Variance')\n",
        "axes[2].set_title('Learned Noise Variances')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-16-ml1s5zpx-7w28u4",
      "metadata": {},
      "source": [
        "## Robustness Analysis\n",
        "\n",
        "Test the model's robustness to different levels of noise to demonstrate the benefits of neuromodulation training.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-17-ml1s5zpx-mssjmo",
      "metadata": {},
      "source": [
        "# Test robustness to varying noise levels\n",
        "noise_scales = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "accuracies = []\n",
        "\n",
        "for scale in noise_scales:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.size(0), -1)\n",
        "            \n",
        "            # Inject fixed-scale noise\n",
        "            if scale > 0:\n",
        "                noise_dict = {}\n",
        "                for i, layer in enumerate(model.get_leaky_layers()):\n",
        "                    noise = torch.randn_like(layer.weight) * scale\n",
        "                    noise_dict[f'layer_{i}'] = noise\n",
        "                model.inject_noise(noise_dict)\n",
        "            \n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            if scale > 0:\n",
        "                model.clear_noise()\n",
        "    \n",
        "    acc = 100. * correct / total\n",
        "    accuracies.append(acc)\n",
        "    print(f'Noise scale {scale:.2f}: Accuracy = {acc:.2f}%')\n",
        "\n",
        "# Plot robustness curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(noise_scales, accuracies, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Noise Scale (Standard Deviation)', fontsize=12)\n",
        "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "plt.title('Model Robustness to Weight Noise', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-18-ml1s5zpx-hemfq9",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated leaky tensors as a model of neuromodulation in deep networks. Key findings:\n",
        "\n",
        "1. The model successfully learns to be robust to weight noise injected at every training step\n",
        "2. The learnable noise model adapts its variance parameters to find optimal noise levels for each layer\n",
        "3. Training with neuromodulation creates networks that maintain performance even when weights are perturbed\n",
        "4. This approach simulates biological neuromodulation where neural systems must operate reliably despite ongoing perturbations\n",
        "\n",
        "The leaky tensor framework provides a principled way to study robustness and neuromodulation in artificial neural networks, with potential applications in creating more robust and adaptive AI systems.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}