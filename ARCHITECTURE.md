# Leaky Tensors: System Architecture & Theory

## ğŸ§  Core Concept

**Leaky Tensors** is a biologically-inspired training paradigm that injects learnable noise into neural network weights during training. This simulates **neuromodulation** â€” the process by which neurotransmitters modulate synaptic connections in biological neural circuits.

### The Key Insight

In biological brains, synaptic weights aren't static. Neurotransmitters like dopamine, serotonin, and acetylcholine continuously modulate connection strengths. Networks trained under this constant perturbation become inherently robust.

We model this by:
1. Adding Gaussian noise to weights at **every forward pass** during training
2. Making the noise variance **learnable** â€” the network learns optimal noise levels
3. Training both the main network and noise model jointly

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           LEAKY TENSORS TRAINING LOOP                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Input     â”‚        â”‚    Noise Model      â”‚        â”‚   Main Network   â”‚
     â”‚   (MNIST)    â”‚        â”‚  (Learnable ÏƒÂ²)     â”‚        â”‚   (LeakyMLP)     â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                           â”‚                            â”‚
            â”‚                           â”‚ Generate Noise             â”‚
            â”‚                           â”‚ Îµ ~ N(0, ÏƒÂ²)               â”‚
            â”‚                           â–¼                            â”‚
            â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
            â”‚                    â”‚  Noise Dict  â”‚                    â”‚
            â”‚                    â”‚  per layer   â”‚                    â”‚
            â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
            â”‚                           â”‚                            â”‚
            â”‚                           â”‚ Inject                     â”‚
            â”‚                           â–¼                            â”‚
            â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
            â”‚              â”‚     WEIGHT + NOISE      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚              â”‚    W' = W + Îµ           â”‚
            â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                           â”‚
            â–¼                           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                                                â”‚
     â”‚                        FORWARD PASS                            â”‚
     â”‚                                                                â”‚
     â”‚   Input â”€â”€â–º [LeakyLinearâ‚] â”€â”€â–º ReLU â”€â”€â–º [LeakyLinearâ‚‚] â”€â”€â–º... â”‚
     â”‚              Wâ‚ + Îµâ‚                     Wâ‚‚ + Îµâ‚‚               â”‚
     â”‚                                                                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Prediction   â”‚
                              â”‚   (logits)    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ CrossEntropy  â”‚â—„â”€â”€â”€â”€ Target Labels
                              â”‚     Loss      â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                    BACKWARD PASS                     â”‚
           â”‚                                                      â”‚
           â–¼                                                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Update Weights â”‚                                  â”‚ Update Noise   â”‚
    â”‚  (Adam, lr=1e-3)â”‚                                  â”‚ Variances      â”‚
    â”‚                 â”‚                                  â”‚ (Adam, lr=1e-4)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Component Details

### 1. LeakyLinear Layer

Custom linear layer that supports noise injection:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LeakyLinear                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters:                                        â”‚
â”‚    â€¢ W: Weight matrix [out_features Ã— in_features]  â”‚
â”‚    â€¢ b: Bias vector [out_features]                  â”‚
â”‚    â€¢ current_noise: Injected noise tensor           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Forward(x):                                        â”‚
â”‚    if training and noise_injected:                  â”‚
â”‚        return x @ (W + noise)áµ€ + b                  â”‚
â”‚    else:                                            â”‚
â”‚        return x @ Wáµ€ + b                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Noise Model

Learns optimal noise variance per layer:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NoiseModel                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Parameters (per layer):                            â”‚
â”‚    â€¢ log_ÏƒÂ²: Log-variance (scalar, learnable)       â”‚
â”‚                                                     â”‚
â”‚  Why log-variance?                                  â”‚
â”‚    â€¢ Ensures ÏƒÂ² is always positive                  â”‚
â”‚    â€¢ More stable gradient flow                      â”‚
â”‚    â€¢ Prevents variance collapse to zero             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  generate_noise():                                  â”‚
â”‚    Ïƒ = exp(0.5 Ã— log_ÏƒÂ²)      # Convert to std     â”‚
â”‚    Ïƒ = clamp(Ïƒ, 1e-6, 0.1)    # Stability bounds   â”‚
â”‚    Îµ = randn(shape) Ã— Ïƒ       # Sample noise       â”‚
â”‚    return Îµ                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. LeakyMLP Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LeakyMLP                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚   Input (784)                                                              â”‚
â”‚       â”‚                                                                    â”‚
â”‚       â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ LeakyLinear   â”‚â”€â”€â”€â–ºâ”‚ ReLU  â”‚â”€â”€â”€â–ºâ”‚  Dropout    â”‚                       â”‚
â”‚   â”‚ (784 â†’ 512)   â”‚    â”‚       â”‚    â”‚   (0.2)     â”‚                       â”‚
â”‚   â”‚   + noiseâ‚€    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚                              â”‚
â”‚                                            â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ LeakyLinear   â”‚â”€â”€â”€â–ºâ”‚ ReLU  â”‚â”€â”€â”€â–ºâ”‚  Dropout    â”‚                       â”‚
â”‚   â”‚ (512 â†’ 256)   â”‚    â”‚       â”‚    â”‚   (0.2)     â”‚                       â”‚
â”‚   â”‚   + noiseâ‚    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚                              â”‚
â”‚                                            â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚ LeakyLinear   â”‚â”€â”€â”€â–º Output (10 classes)                               â”‚
â”‚   â”‚ (256 â†’ 10)    â”‚                                                       â”‚
â”‚   â”‚   + noiseâ‚‚    â”‚                                                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Mathematical Formulation

### Standard Forward Pass
$$y = f(Wx + b)$$

### Leaky Forward Pass (Training)
$$y = f((W + \epsilon)x + b), \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)$$

Where:
- $W$ = weight matrix (learned)
- $\epsilon$ = noise matrix (sampled each forward pass)
- $\sigma^2$ = variance (learned by noise model)

### Optimization Objective

We jointly minimize:

$$\mathcal{L}_{total} = \mathbb{E}_{\epsilon}[\mathcal{L}_{CE}(f_\theta(x; W + \epsilon), y)]$$

This expectation forces the network to find weight configurations that are robust across a distribution of perturbations.

---

## ğŸ§¬ Biological Inspiration

| Biological System | Leaky Tensors Analog |
|-------------------|----------------------|
| Synaptic Weight | Weight matrix W |
| Neuromodulator (dopamine, etc.) | Noise Îµ |
| Neuromodulator concentration | Variance ÏƒÂ² |
| Synaptic plasticity | Gradient updates to W |
| Homeostatic regulation | Learned variance adaptation |

### Why This Matters

1. **Robustness**: Networks trained with noise perturbations generalize better to unseen conditions
2. **Biological Plausibility**: More accurately models noisy biological computation
3. **Regularization**: Acts as implicit regularization, similar to dropout but at the weight level
4. **Adversarial Robustness**: Networks become more resistant to adversarial weight perturbations

---

## ğŸ“Š Training Flow

```
For each epoch:
    For each batch (x, y):
        â”‚
        â”œâ”€â–º noise_model.generate_noise()     # Sample Îµ ~ N(0, ÏƒÂ²)
        â”‚
        â”œâ”€â–º model.inject_noise(noise_dict)   # W' = W + Îµ
        â”‚
        â”œâ”€â–º output = model(x)                # Forward with noisy weights
        â”‚
        â”œâ”€â–º loss = CrossEntropy(output, y)   # Compute loss
        â”‚
        â”œâ”€â–º loss.backward()                  # Backprop through both
        â”‚
        â”œâ”€â–º model_optimizer.step()           # Update W
        â”‚
        â”œâ”€â–º noise_optimizer.step()           # Update ÏƒÂ²
        â”‚
        â””â”€â–º model.clear_noise()              # Reset for next batch
```

---

## ğŸ“ˆ Expected Behavior

### Noise Variance Evolution
- **Early training**: Network may benefit from higher noise (exploration)
- **Later training**: Noise variance typically decreases (exploitation)
- **Per-layer differences**: Different layers may learn different optimal variances

### Robustness Characteristics
- Model trained with neuromodulation should degrade gracefully under inference-time noise
- Standard models (no noise training) collapse quickly when weights are perturbed

---

## ğŸ”— Connections to Related Work

| Technique | Relationship to Leaky Tensors |
|-----------|------------------------------|
| **Dropout** | Noise on activations vs. noise on weights |
| **Weight Decay** | Static regularization vs. dynamic perturbation |
| **Bayesian Neural Networks** | Full posterior vs. learned noise variance |
| **Shake-Shake Regularization** | Similar concept for residual branches |
| **Noisy Networks (NoisyNet)** | Exploration in RL via weight noise |

---

## ğŸš€ Usage

```python
# Create leaky model
model = create_model('mlp', input_dim=784, hidden_dims=[512, 256], output_dim=10)

# Create noise model with proper layer shapes
layer_shapes = {f'layer_{i}': l.weight.shape for i, l in enumerate(model.get_leaky_layers())}
noise_model = NoiseModel(layer_shapes)

# Training loop injects noise at each step
noise_dict = noise_model.generate_noise()
model.inject_noise(noise_dict)
output = model(x)
# ... compute loss and backprop ...
model.clear_noise()
```

---

*This architecture document accompanies the Leaky Tensors notebook demonstrating neuromodulation in deep networks.*
